import re
import requests
import subprocess
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from pymongo import MongoClient

def find_sitemap(domain):
    common_locations = ['/sitemap.xml', '/sitemap_index.xml', '/sitemap1.xml', '/sitemap', '/sitemap/', '/sitemap.php',
                        '/sitemap.txt', '/sitemap.xml.gz', '/post-sitemap.xml', '/page-sitemap.xml', '/sitemap-index.xml',
                        '/sitemapindex.xml', '/sitemap_index.xml.gz', '/sitemap/index.xml', '/rss/', '/rss.xml',
                        '/atom.xml', '/sitemap.html', 'sitemap_index.html', '/sitemap_index']

    for location in common_locations:
        try:
            url = urljoin(f'http://{domain}', location)
            response = requests.get(url, timeout=5)
            if response.status_code == 200 and ('404' not in response.text) and ('not found' not in response.text):
                return url
        except requests.Timeout:
            print(f"Timeout error for URL: {domain}")
        except requests.RequestException as e:
            print(f"Request error for URL {domain}: {e}")

    # also search the robots file
    try:
        url = urljoin(f'http://{domain}', '/robots.txt')
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('loc')
            for link in links:
                if ("sitemap" in link) or ("site_map" in link):
                    return link
    except requests.Timeout:
        print(f"Timeout error for URL: {domain}")
    except requests.RequestException as e:
        print(f"Request error for URL {domain}: {e}")

    return None

def run_scrapy_spider(sitemap_url):
    # Run the Scrapy spider for the given sitemap URL and capture the output
    result = subprocess.run(['scrapy', 'crawl', 'linkcounter', '-a', f'sitemap_url={sitemap_url}'], capture_output=True, text=True)
    return result.stdout


def extract_terminal_links_count(log_output):
    count = 0
    terminal_links_matches = re.findall(r"Number of terminal links: (\d+)", log_output)

    for match in terminal_links_matches:
        count += int(match)

    return count

def extract_sitemap_info(domain, sitemap_url):
    response = requests.get(sitemap_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        if ".xml" in sitemap_url:
            map_type = 'xml'
            soup = BeautifulSoup(response.text, 'xml')

        elif ".html" in sitemap_url:
            map_type = 'html'
        elif ".php" in sitemap_url:
            map_type = 'php'
        elif ".txt" in sitemap_url:
            map_type = 'txt'
        else:
            map_type = 'other'

        # Extracting the number of links
        links = soup.find_all('loc')
        test_link_count = len(links)

        if test_link_count > 0: # dont run sitemap spider if no links on the sitemap
            output = run_scrapy_spider(sitemap_url)
            num_links = extract_terminal_links_count(output)
            if num_links is None:
                num_links = 0
        else:
            num_links = 0

        # Extracting the number of non-link items (e.g., images)
        images = soup.find_all('image')
        num_images = len(images)

        # Extracting the sitemap size
        sitemap_size = len(response.content)

        # Checking for the frequency of nested sitemaps
        is_nested_sitemap = "sitemap_index" in sitemap_url.lower()

        return {
            'url': domain,
            'has_sitemap': True,
            'sitemap_loc': sitemap_url,
            'sitemap_type': map_type,
            'num_links': num_links,
            'num_images': num_images,
            'sitemap_size': sitemap_size,
            'is_nested_sitemap': is_nested_sitemap
        }

    return None


if __name__ == "__main__":
    # Connect to MongoDB
    client = MongoClient('mongodb+srv://user123:pass123@crawler.rwsmu5b.mongodb.net/?retryWrites=true&w=majority')
    db = client['Crawler']
    collection = db['run2']

    try:
        client.admin.command('ping')
        print("Pinged your deployment. You successfully connected to MongoDB!")
    except Exception as e:
        print(e)

    # Read domain names from the file generated by the spider
    with open('domain_names1.txt', 'r') as f:
        domain_names = [line.strip() for line in f]

    for domain in domain_names:
        sitemap_url = find_sitemap(domain)
        if sitemap_url:
            print(f"Sitemap found for {domain}: {sitemap_url}")

            # Extract sitemap information
            sitemap_info = extract_sitemap_info(domain, sitemap_url)

            if sitemap_info:
                # Store the information in MongoDB
                collection.insert_one(sitemap_info)
                print(f"Sitemap information stored in MongoDB.")
        else:
            default_insert = {
                'url': domain,
                'has_sitemap': False,
                'sitemap_loc': None,
                'sitemap_type': None,
                'num_links': None,
                'num_images': None,
                'sitemap_size': None,
                'is_nested_sitemap': None
            }
            collection.insert_one(default_insert)
            print(f"No sitemap found for {domain}")

    # Close the MongoDB connection
    client.close()
